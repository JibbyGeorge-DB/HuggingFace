{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1V85Zds5kEkC0vHKPe0xb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JibbyGeorge-DB/HuggingFace/blob/main/HuggingFace_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFace Pipelines\n"
      ],
      "metadata": {
        "id": "ILsjhXtNvV6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade datasets==3.6.0\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FCKpEqnqvb1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the GPU - it should be a Tesla T4\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "  if gpu_info.find('Tesla T4') >= 0:\n",
        "    print(\"Success - Connected to a T4\")\n",
        "  else:\n",
        "    print(\"NOT CONNECTED TO A T4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUohjonVvqwe",
        "outputId": "25ce1a89-3865-41c7-ce76-60359593ee5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan  8 22:34:46 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Success - Connected to a T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import torch                      # The core PyTorch library used for tensor computations and running the deep learning models.\n",
        "from google.colab import userdata # A Colab-specific tool used to securely access your stored \"Secrets\" (like API keys) without hardcoding them.\n",
        "from huggingface_hub import login # Functionality to authenticate your environment with Hugging Face to access private or gated models (like Llama or SD3).\n",
        "from transformers import pipeline # The easiest way to use pre-trained models for NLP or Audio tasks (like speech-to-text or sentiment analysis).\n",
        "from diffusers import DiffusionPipeline # The primary class used to load and run image-generation models like Stable Diffusion or Flux.\n",
        "from datasets import load_dataset # A utility to quickly download and prepare large-scale community datasets for training or evaluation.\n",
        "import soundfile as sf            # A library for reading and writing audio files, often used to save generated speech or music to your disk.\n",
        "from IPython.display import Audio # A helper that creates an interactive audio player directly inside your notebook cells."
      ],
      "metadata": {
        "id": "Uj4az-iBweWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While \"Sentiment Analysis\" is the most common use case, it is technically a subset of the broader Text Classification task in the Transformers library."
      ],
      "metadata": {
        "id": "SXC-NFA13HKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "#Model will be automatically selected in Colab if no model is mentioned\n",
        "\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\",  device_map=\"auto\")\n",
        "result = sentiment_analyzer(\"Im super excited to be here\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3FNJgmQaxTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Generation\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n",
        "\n",
        "user_message = \"\"\"\n",
        "What are some fun activities I can do in Melbourne, Australia?\n",
        "\"\"\"\n",
        "result = pipe(user_message)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "1uksNlpj7d6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translation\n",
        "# Translating to French\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device_map=\"auto\")\n",
        "text_translated = translator(\"Hello Good Morning\",\n",
        "                             src_lang=\"eng_Latn\",\n",
        "                             tgt_lang=\"fra_Latn\")\n",
        "print (text_translated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihQhAc1b5V4k",
        "outputId": "b4c7a694-746e-44e9-9bc1-780e770ec784"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': 'Buenos días.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Summarization\n",
        "summarizer = pipeline(\"summarization\", device_map=\"auto\")\n",
        "text = \"\"\"\n",
        "Self-hosting large language models (LLMs) on Kubernetes is gaining momentum among organizations with inference workloads at scale, such as batch processing, chatbots, agents, and AI-driven applications. These organizations often have access to commercial-grade GPUs and are seeking alternatives to costly per-token API pricing models, which can quickly scale out of control. Many also require the ability to fine-tune or customize their models, a capability typically restricted by closed-source API providers. Additionally, companies handling sensitive or proprietary data - especially in regulated sectors such as finance, healthcare, or defense - prioritize self-hosting to maintain strict control over data and prevent exposure through third-party systems.\n",
        "To address these needs and more, the Kubernetes AI Toolchain Operator (KAITO), a Cloud Native Computing Foundation (CNCF) Sandbox project, simplifies the process of deploying and managing open-source LLM workloads on Kubernetes. KAITO integrates with vLLM, a high-throughput inference engine designed to serve large language models efficiently. vLLM as an inference engine helps reduce memory and GPU requirements without significantly compromising accuracy.\n",
        "Built on top of the open-source KAITO project, the AI toolchain operator managed add-on offers a modular, plug-and-play setup that allows teams to quickly deploy models and expose them via production-ready APIs. It includes built-in features like OpenAI-compatible APIs, prompt formatting, and streaming response support. When deployed on an AKS cluster, KAITO ensures data stays within your organization’s controlled environment, providing a secure, compliant alternative to cloud-hosted LLM APIs.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXXV_tXIein",
        "outputId": "372cf023-0a00-42d5-e56e-7e76c40a4760"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Self-hosting large language models (LLMs) on Kubernetes is gaining momentum among organizations with inference workloads at scale . KAITO integrates with vLLM, a high-throughput inference engine designed to\n"
          ]
        }
      ]
    }
  ]
}